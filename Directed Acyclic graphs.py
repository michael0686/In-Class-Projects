# -*- coding: utf-8 -*-
"""
Created on Tue Apr  9 13:34:09 2024

@author: nkc06
"""

#Homework is at the bottom of this notebook

from pgmpy.estimators import PC
import networkx as nx
import matplotlib.pyplot as plt
from matplotlib.patches import ArrowStyle
plt.rcParams.update({"font.size":26})
G = nx.DiGraph()
G.add_edges_from([("X","Y"),("Y","Z")])
pos = nx.spring_layout(G)#, k = 5/(len(sig_corr.keys())**.5))

fig,ax  = plt.subplots(figsize = (20,10))
# plt.title(title, fontsize = 30)
nx.draw_networkx(G, pos,node_size = 1200,
     with_labels=True,  arrows=True,
     font_color = "white",
     font_size = 26, alpha = 1,
     width = 1, edge_color = "C1",
     arrowstyle=ArrowStyle(
         "Fancy, head_length=3, head_width=1.5, tail_width=.1"),
                ax = ax)
ax.set_title("Chain")

import random
import numpy as np
import pandas as pd
length = 100000
cols = ["X", "Y", "Z"]
chain = {col:[] for col in cols}
for i in range(length):
    chain["X"].append(np.random.normal(0, 1))
    chain["Y"].append(chain["X"][-1] * -1 + np.random.normal(0, 2))
    chain["Z"].append(chain["Y"][-1] * .5 + np.random.normal(0, 1))
chain = pd.DataFrame(chain)
chain.tail()
#Here we simulate 'X' by drawing it from a normal distribution with mean 0 and std. dev 1. 'Y' is generated by taking 
#the inverse of X and adding values drawn from a normal distribution. 'Z' is generated as a function of 'Y'. This
#simulation allows us to create data that contains a var which is exogenous 'X' and two variables that are endogenous
#'Y' and 'Z'. 

import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

fig,ax  = plt.subplots(figsize = (20,10))
# plt.title(title, fontsize = 30)
nx.draw_networkx(G, pos,node_size = 1200,
     with_labels=True,  arrows=True,
     font_color = "white",
     font_size = 26, alpha = 1,
     width = 1, edge_color = "C1",
     arrowstyle=ArrowStyle(
         "Fancy, head_length=3, head_width=1.5, tail_width=.1"),
                ax = ax)
ax.set_title("Chain")

import random
import numpy as np
import pandas as pd
length = 100000
cols = ["X", "Y", "Z"]
chain = {col:[] for col in cols}
for i in range(length):
    chain["X"].append(np.random.normal(0, 1))
    chain["Y"].append(chain["X"][-1] * -1 + np.random.normal(0, 2))
    chain["Z"].append(chain["Y"][-1] * .5 + np.random.normal(0, 1))
chain = pd.DataFrame(chain)
chain
#In this chain we can see that var x is exogenous while var. z and var y.
#are endogenous. Though x is not directly correlated with z it is indirectly
#correlated with z through y

chain.corr()

def formatted_scatter_matrix(data, c = "C0", cmap = "viridis", alpha = .1, pp=False):  
    # Create a figure showing scatterplots given in scatter_cats  
    fig_len = 15  
    fig, ax = plt.subplots(figsize = ((fig_len, fig_len)))  
    # Use fig_len to dictate fig_size, adjust size of font, size of dots, etc...  
    num_vars = len(data.keys())  
    fontsize = 65 / num_vars  
    plt.rcParams.update({'font.size': fontsize})  
    pd.plotting.scatter_matrix(data, c = c, alpha = alpha, s = 200 / num_vars, ax=ax)  
    # tight layout improves layout of text and plots in the figure  
    plt.tight_layout()  
    plt.show()  
    if pp != False:
        pp.savefig(fig, bbox_inches = "tight")  
    plt.close()  

formatted_scatter_matrix(chain, alpha = .025)

def corr_matrix_heatmap(data, pp):  
    #Create a figure to visualize a corr matrix  
    fig, ax = plt.subplots(figsize=(20,20))  
    # use ax.imshow() to create a heatmap of correlation values  
    # seismic mapping shows negative values as blue and positive values as red  
    im = ax.imshow(data, norm = plt.cm.colors.Normalize(-1,1), cmap = "seismic")  
    # create a list of labels, stacking each word in a label by replacing " "  
    # with "\n"  
    labels = data.keys()  
    num_vars = len(labels)  
    tick_labels = [lab.replace(" ", "\n") for lab in labels]  
    # adjust font size according to the number of variables visualized  
    tick_font_size = 120 / num_vars  
    val_font_size = 200 / num_vars  
    plt.rcParams.update({'font.size': tick_font_size}) 
    # prepare space for label of each column  
    x_ticks = np.arange(num_vars)  
    # select labels and rotate them 90 degrees so that they are vertical  
    plt.xticks(x_ticks, tick_labels, fontsize = tick_font_size, rotation = 90)  
    # prepare space for label of each row  
    y_ticks = np.arange(len(labels))  
    # select labels  
    plt.yticks(y_ticks, tick_labels, fontsize = tick_font_size)  
    # show values in each tile of the heatmap  
    for i in range(len(labels)):  
        for j in range(len(labels)):  
            text = ax.text(i, j, str(round(data.values[i][j],2)),  
                           fontsize= val_font_size, ha="center",   
                           va="center", color = "w")  
    #Create title with Times New Roman Font  
    title_font = {"fontname":"Times New Roman"}  
    plt.title("Correlation", fontsize = 50, **title_font)  
    #Call scale to show value of colors 
    cbar = fig.colorbar(im)
    plt.show()
    pp.savefig(fig, bbox_inches="tight")
    plt.close()
    
corr_matrix_heatmap(chain.corr(), 
                    pp = False)

import statsmodels.api as sm

YZc = chain[["Y","Z"]]
YZc["Constant"] = 1
X = chain[["X"]]
# pass y_var as list for consistent structure
model = sm.OLS(X, YZc)
results = model.fit()
results.summary()

residuals = {}
def get_residuals(df):
    for y_var in df.keys():
        X_vars = list(df.keys())
        X_vars.remove(y_var)
        X = df[X_vars]
        # Initial estimate should include constant
        #   This won't be the case we regress the errors
        X["Constant"] = 1
        # pass y_var as list for consistent structure
        y = df[[y_var]]
        model = sm.OLS(y, X)
        results = model.fit()
        residuals["$\\epsilon_{" + y_var + "}$"] = results.resid
    return pd.DataFrame(residuals)
residuals = get_residuals(chain)
residuals
#In this function we generalize the computation of the residuals using a 
#for loop that iterates over the variables in x_vars and regresses each of
#them onto the y_var to obtain the residuals from the model results



def calculate_partial_corr(residuals):
    pcorr = residuals.corr() * -1
    #correct the sign of the corr. coeffecients
    for x in residuals.keys():        
        for y in residuals.keys():
            if x == y:
                pcorr[y][x] = 1
                #variables are perfectly correlated with themselves
    return pcorr
# use variable names since are correcting correlations of residuals to reflect
#  partial correlation of respective variables 
chain_pcorr = calculate_partial_corr(residuals.rename(columns = {r:r[-3] for r in residuals.keys()}))
chain_pcorr.round(3)

!pip install pingouin

import pingouin
pingouin.partial_corr(data=chain, x='X', y='Z', covar='Y').round(3)

corr_matrix_heatmap(chain.corr(), pp = None)


fig,ax = plt.subplots(figsize = (20,10))
chain.plot.scatter(x = "X", y = "Y", c = "Z", cmap = "viridis", s = 5, ax = ax)
ax.set_title("$X \\rightarrow Y \\rightarrow Z$")
for i in range(-5,6): ax.axhline(i, c = "k")

G = nx.DiGraph()
G.add_edges_from([("X","Y"),("X","Z")])
pos = nx.spring_layout(G)#, k = 5/(len(sig_corr.keys())**.5))

fig,ax  = plt.subplots(figsize = (20,10))
# plt.title(title, fontsize = 30)
nx.draw_networkx(G, pos,node_size = 1200,
     with_labels=True,  arrows=True,
     font_color = "white",
     font_size = 26, alpha = 1,
     width = 1, edge_color = "C1",
     arrowstyle=ArrowStyle(
         "Fancy, head_length=3, head_width=1.5, tail_width=.1"),
                ax = ax)
ax.set_title("Fork")


fork = {col:[] for col in cols}
for i in range(length):
    fork["X"].append(np.random.normal(0, 1))
    fork["Y"].append(fork["X"][-1] + np.random.normal(0, 1))
    fork["Z"].append(fork["X"][-1] * 2 + np.random.normal(0, 1))
fork = pd.DataFrame(fork)
fork
#Here we simulate correlation between x and y and x and z by including x 
#in the creation of both y and z

fork.corr()
#Returns correlation coeffecients for vars. in 'fork'

XZc = fork[["X","Z"]]
XZc["Constant"] = 1
Y = fork[["Y"]]
# pass y_var as list for consistent structure
model = sm.OLS(Y, XZc)
results = model.fit()
results.summary()
#Regress X and Z on Y. We can use OLS regression to analyze the beta coeffecients of the regression. The beta
#coeffecients should support the intution we are building in our scatterplots.

pingouin.partial_corr(data=fork, x='Y', y='Z', covar='X').round(3)

fig,ax = plt.subplots(figsize = (20,10))
fork.plot.scatter(x = "Y", y = "X", c = "Z",s = 5, cmap = "viridis", ax = ax)
ax.set_title("$Y \\leftarrow X \\rightarrow Z$")
for i in range(-3,4): ax.axhline(i, c = "k")

collider = {col:[] for col in cols}
for i in range(length):
    collider["Y"].append(np.random.normal(0, 1))
    collider["Z"].append(np.random.normal(0, 1))
    collider["X"].append(collider["Y"][-1] * -.5 + collider["Z"][-1] * .5 + np.random.normal(0, 1))
collider = pd.DataFrame(collider)
collider

collider.corr()

YZc = collider[["Y","Z"]]
YZc["Constant"] = 1
X = collider[["X"]]
# pass y_var as list for consistent structure
model = sm.OLS(X, YZc)
results = model.fit()
results.summary()

fig,ax = plt.subplots(figsize = (20,10))
collider.plot.scatter(x = "Y", y = "Z", c = "X",s = 5, cmap = "viridis", ax = ax)
ax.set_title("$Y \\rightarrow X \\leftarrow Z$")
#Using the scatterplot below it appears z is positively correlated with x 
#because values of x are above the average of x when values of z are above
#the average value of z. Conversely, y and x appear negatively correlated
#because values for x on are lower than the average value of x when values
#of y are higher than the average value of y. The beta coeffecients from
#our regression support this intuition.

G = nx.DiGraph()
G.add_edges_from([("Q","X"),("Q","Y"), ("X","Z"),("Y","Z")])
pos = nx.spring_layout(G)#, k = 5/(len(sig_corr.keys())**.5))

fig,ax  = plt.subplots(figsize = (20,10))
# plt.title(title, fontsize = 30)
nx.draw_networkx(G, pos,node_size = 1200,
     with_labels=True,  arrows=True,
     font_color = "white",
     font_size = 26, alpha = 1,
     width = 1, edge_color = "C1",
     arrowstyle=ArrowStyle(
         "Fancy, head_length=3, head_width=1.5, tail_width=.1"),
                ax = ax)
ax.set_title("Backdoor Criterion")

cols = ["Q", "X", "Y", "Z"]
mu = 0
sigma = 1


backdoor = {col:[] for col in cols}
for i in range(length):
    backdoor["Q"].append(np.random.normal(mu, sigma))
    backdoor["X"].append(3 * backdoor["Q"][-1]  + np.random.normal(mu, sigma ))
    backdoor["Y"].append(backdoor["Q"][-1] * -1 +  np.random.normal(mu, sigma))
    backdoor["Z"].append(1.5 * backdoor["X"][-1] -  backdoor["Y"][-1] * 2 +  np.random.normal(mu,  sigma))

backdoor = pd.DataFrame(backdoor)
backdoor

import statsmodels.api as sm

QXYc = backdoor[["Q", "X", "Y"]]
QXYc["Constant"] = 1
Z = backdoor[["Z"]]
# pass y_var as list for consistent structure
model = sm.OLS(Z, QXYc)
results = model.fit()
results.summary()

XYc = backdoor[["X", "Y"]]
model = sm.OLS(Z, XYc)
results = model.fit()
results.summary()

import random
import numpy as np
import pandas as pd

cols = ["Q", "X", "Y", "Z"]
mu = 0
sigma = 1


frontdoor = {col:[] for col in cols}
for i in range(length):Xc = frontdoor[["X"]]
Xc["Constant"] = 1
Z = frontdoor[["Z"]]
# pass y_var as list for consistent structure
model = sm.OLS(Z, Xc)
results = model.fit()
results.summary()

    frontdoor["Q"].append(np.random.normal(mu, sigma))
    frontdoor["X"].append(10 * frontdoor["Q"][-1]  + np.random.normal(mu, sigma ))
    frontdoor["Y"].append(frontdoor["X"][-1] * 2 +  np.random.normal(mu, sigma))
    frontdoor["Z"].append(frontdoor["Q"][-1] * -1 +  frontdoor["Y"][-1] +  np.random.normal(mu,  sigma))

frontdoor = pd.DataFrame(frontdoor)
frontdoor

G = nx.DiGraph()
G.add_edges_from([("Q","X"),("Q","Z"), ("X","Y"),("Y","Z")])
pos = nx.spring_layout(G)#, k = 5/(len(sig_corr.keys())**.5))

fig,ax  = plt.subplots(figsize = (20,10))
# plt.title(title, fontsize = 30)
nx.draw_networkx(G, pos,node_size = 1200,
     with_labels=True,  arrows=True,
     font_color = "white",
     font_size = 26, alpha = 1,
     width = 1, edge_color = "C1",
     arrowstyle=ArrowStyle(
         "Fancy, head_length=3, head_width=1.5, tail_width=.1"),
                ax = ax)
ax.set_title("Frontdoor Criterion")

import statsmodels.api as sm

Xc = frontdoor[["X"]]
Xc["Constant"] = 1
Z = frontdoor[["Z"]]
# pass y_var as list for consistent structure
model = sm.OLS(Z, Xc)
results = model.fit()
results.summary()

import statsmodels.api as sm

XYc = frontdoor[["X", "Y"]]
XYc["Constant"] = 1
Z = frontdoor[["Z"]]
# pass y_var as list for consistent structure
model = sm.OLS(Z, XYc)
results = model.fit()
results.summary()

QXc = frontdoor[["Q","X"]]
QXc["Constant"] = 1
Z = frontdoor[["Z"]]
# pass y_var as list for consistent structure
model = sm.OLS(Z, QXc)
results = model.fit()
results.summary()

import statsmodels.api as sm

QXYc = frontdoor[["Q","X","Y"]]
QXYc["Constant"] = 1
Z = frontdoor[["Z"]]
# pass y_var as list for consistent structure
model = sm.OLS(Z, QXYc)
results = model.fit()
results.summary()

length = 100000
cols = ["P", "Q", "X", "Y", "Z"]
mu = 0
sigma = 5


lst_dct = {col:[] for col in cols}
for i in range(length):
    lst_dct["P"].append(np.random.normal(mu, .5 * sigma))
    lst_dct["Q"].append(np.random.normal(mu, sigma))
    lst_dct["X"].append(3 * lst_dct["Q"][-1]  + np.random.normal(mu, sigma ))
    lst_dct["Y"].append(lst_dct["Q"][-1] * -1 +  np.random.normal(mu, sigma))
    lst_dct["Z"].append(
        lst_dct["P"][-1] * 2 +  1.5 * lst_dct["X"][-1] -  lst_dct["Y"][-1] * 2 +  np.random.normal(mu,  sigma))

df = pd.DataFrame(lst_dct)
df

undirected_graph = {key:[] for key in df.keys()}
for x in undirected_graph:
    remaining_vars = [y for y in df.keys() if y != x]
    for y in remaining_vars:
        undirected_graph[x].append(y)

undirected_graph 
#this loop generates a dictionary that uses the variable itself as the key
#and the values associated with each key are the all the other variables
#that the key var. could be correlated with

import copy
p_val = .005
def build_skeleton(df, undirected_graph):    
    def check_remaining_controls(control_vars, undirected_graph, x, y, controls_used) :
        for c_var in control_vars:
            # set c_used every time use cycle through a new control
            #  the program will then iterate through remaining controls
            #  until statistical significance is broken
            c_used = copy.copy(controls_used)
            if y in undirected_graph[x]:

                c_used.append(c_var)
                test = df.partial_corr(x = x, y = y, covar=c_used,
                                      method = "pearson")
                if test["p-val"].values[0] > p_val: 

                    undirected_graph[x].remove(y)
                    #breakout of the for 
                    break
                else:
                    remaining_controls = copy.copy(control_vars)
                    remaining_controls.remove(c_var)
                    # recursive function that iterates through remaining variables 
                    #  uses them as controls statistical significance holds without them,
                    #  otherwise break
                    check_remaining_controls(remaining_controls, undirected_graph, x, y, c_used)
                
    for x in df.keys():
        ys = undirected_graph[x]
        for y in df.keys():
            if x != y:
            # first check for correlation with no controls
                test = df.partial_corr(x = x, 
                                       y = y, 
                                       covar = None,
                                       method = "pearson") 
                if test["p-val"].values[0] > p_val:
                    undirected_graph[x].remove(y)
            # if correlated check for deseparation controlling for other variables
                else:
                    control_vars = [z for z in df.keys() if z != y and z != x]
                    check_remaining_controls(control_vars, undirected_graph, x, y, [])
    return undirected_graph

undirected_graph = build_skeleton(df, undirected_graph)                                   
undirected_graph
#This function builds a casual skeleton using correlation coeffecients and p-values. This function returns the
#variables that a given variable is directly correlated with.

import matplotlib.pyplot as plt
import networkx as nx
def graph_DAG(undirected_graph, df, title = "DAG Structure"):
    
    # generate partial correlation matrix to draw values from
    # for graph edges
    graph = nx.Graph()
    edges = []
    edge_labels = {}
    for key in undirected_graph:
        for key2 in undirected_graph[key]:
            if (key2, key) not in edges:
                edge = (key.replace(" ","\n"), key2[0].replace(" ","\n"))
                edges.append(edge)

    # edge format: ("i", "j") --> from node i to node j
    graph.add_edges_from(edges)
    color_map = ["C0" for g in graph]

    fig, ax = plt.subplots(figsize = (20,12))
    graph.nodes()
    plt.tight_layout()
    pos = nx.spring_layout(graph)#, k = 5/(len(sig_corr.keys())**.5))

    plt.title(title, fontsize = 30)
    nx.draw_networkx(graph, pos, node_color=color_map, 
                     node_size = 1000,
                     with_labels=True,  arrows=False,
                     font_size = 20, alpha = 1,
                     font_color = "white",
                     ax = ax)

    plt.axis("off")
    plt.savefig("g1.png", format="PNG")
    plt.show()

graph_DAG(undirected_graph, df, title = "Undirected Graph with Partial Correlations\nfrom Full Set of Controls")

from pgmpy.estimators import PC

c = PC(df)
max_cond_vars = len(df.keys()) - 2


model = c.estimate(return_type = "dag",variant= "parallel",#"orig", "stable"
                   significance_level = p_val, 
                   max_cond_vars = max_cond_vars, ci_test = "pearsonr")
edges = model.edges()

def graph_DAG(edges, df, title = ""):
    graph = nx.DiGraph()
    
    ############ Add ############
    edge_labels = {}
    for edge in edges:
        controls = [key for key in df.keys() if key not in edge]
        controls = list(set(controls))
        keep_controls = []
        for control in controls:
            # Check if the caused variable is also a caused variable in any other links
            control_edges = [ctrl_edge for ctrl_edge in edges if control == ctrl_edge[0] ]
            if (control, edge[1]) in control_edges:
                keep_controls.append(control)                
        pcorr = df[[edge[0], edge[1]]+keep_controls].pcorr()
        edge_labels[edge] = str(round(pcorr[edge[0]].loc[edge[1]],2))
####### End Add ########
    graph.add_edges_from(edges)
    color_map = ["C0" for g in graph]
    fig, ax = plt.subplots(figsize = (20,12))
    graph.nodes()
    plt.tight_layout()
    pos = nx.spring_layout(graph)
    plt.title(title, fontsize = 30)
    nx.draw_networkx(graph, pos, node_color = color_map, node_size = 1200,
        with_labels = True, arrows = True,
        font_color = "white",
        font_size = 26, alpha = 1,
        width = 2, edge_color = "C1",
        arrowstyle = ArrowStyle("Fancy, head_length=1.5, head_width=1.5, tail_width=0.1"),
        ax = ax )
################# Add ####################
    nx.draw_networkx_edge_labels(graph,pos,
                                edge_labels=edge_labels,
                                font_color='green',
                                font_size=20)
graph_DAG(edges, df, title = "Directed Acyclic Graph")

#HomeWork

import pandas as pd
hw_data = pd.read_excel("EFWAndRGDP (1).xlsx")

hw_data.sort_index(inplace = True)

print(hw_data.keys())

cols = ["Size of Government", "Legal System and Property Rights", "Sound Money", "Freedom to Trade Internationally", "Regulation",'RGDP Per Capita']

data = hw_data[cols]
data.keys()
data

data = data.dropna(axis=0)
data.tail
length = len(data.values)
cols_data = list(data.keys())

undirected_graph = {key:[] for key in data.keys()}
for key in undirected_graph:
    remaining_vars = [item for item in cols_data if item != key]  
    undirected_graph[key] = remaining_vars
print(undirected_graph)

undirected_graph_skeleton = build_skeleton(data, undirected_graph)
undirected_graph_skeleton

c = PC(data)
max_cond_vars = len(df.keys()) - 2


model = c.estimate(return_type = "dag",variant= "parallel",#"orig", "stable"
                   significance_level = p_val, 
                   max_cond_vars = max_cond_vars, ci_test = "pearsonr")
edges = model.edges()

graph_DAG(edges, data, title = "Directed Acyclic Graph")

corr_matrix_heatmap(data.corr(), pp = False)

